{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR7vp0ebSLAh3/vXTMYZUP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mr-houngbo/respire/blob/main/prediction/predict_iqa_esmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teR3-EJiwJbX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KWcctNSAwTHH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "import gdown\n",
        "\n",
        "# ID des Datasets et noms des outputs\n",
        "file_ids = [\"1BZazndHP417b_nUGeSxmB-wdaYDb6Ngk\"]\n",
        "outputs = [\"iqa-164928.csv\"]\n",
        "\n",
        "\n",
        "for i,file_id in enumerate(file_ids):\n",
        "  url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "  output = outputs[i]\n",
        "  gdown.download(url, output, quiet=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3K2YF-swTDo",
        "outputId": "2a63853d-6f15-4125-a533-6f8e7df5a0b0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BZazndHP417b_nUGeSxmB-wdaYDb6Ngk\n",
            "To: /content/iqa-164928.csv\n",
            "100%|██████████| 427/427 [00:00<00:00, 620kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "import gdown\n",
        "\n",
        "lien = \"https://drive.google.com/file/d//view?usp=sharing\"\n",
        "\n",
        "\n",
        "# ID du Dataset\n",
        "file_id = \"1b4sKColUCmG2LhQO54tRA2_okXiTvR8I\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = \"164928.csv\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Rfu3SBF1we3x",
        "outputId": "70148a2e-6b00-4c95-9952-25b675fad4bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1b4sKColUCmG2LhQO54tRA2_okXiTvR8I\n",
            "To: /content/164928.csv\n",
            "100%|██████████| 3.62k/3.62k [00:00<00:00, 3.84MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'164928.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_iqa_164928 = pd.read_csv(\"iqa-164928.csv\")\n",
        "esmt = pd.read_csv(\"164928.csv\")"
      ],
      "metadata": {
        "id": "JrdPgZjpwTB0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n2xpHoSbwZP5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Harmoniser : enlever timezone\n",
        "esmt['UTC Date/Time'] = pd.to_datetime(esmt['UTC Date/Time']).dt.tz_localize(None)\n",
        "df_iqa_164928['date'] = pd.to_datetime(df_iqa_164928['date'])\n",
        "\n",
        "# Merge\n",
        "merged = pd.merge(\n",
        "    esmt,\n",
        "    df_iqa_164928,\n",
        "    left_on='UTC Date/Time',\n",
        "    right_on='date',\n",
        "    how='inner'\n",
        ").drop(columns=['date'])\n",
        "\n",
        "print(merged.info())\n",
        "print(merged.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaA4saBAwZMj",
        "outputId": "9a870a51-ab59-4b7e-ec44-a5dd5a12bc40"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18 entries, 0 to 17\n",
            "Data columns (total 25 columns):\n",
            " #   Column                      Non-Null Count  Dtype         \n",
            "---  ------                      --------------  -----         \n",
            " 0   Location ID                 18 non-null     int64         \n",
            " 1   Location Name               18 non-null     object        \n",
            " 2   Location Group              0 non-null      float64       \n",
            " 3   Location Type               18 non-null     object        \n",
            " 4   Sensor ID                   18 non-null     object        \n",
            " 5   Place Open                  18 non-null     bool          \n",
            " 6   Local Date/Time             18 non-null     object        \n",
            " 7   UTC Date/Time               18 non-null     datetime64[ns]\n",
            " 8   # of aggregated records     18 non-null     int64         \n",
            " 9   PM2.5 (μg/m³) raw           18 non-null     float64       \n",
            " 10  PM2.5 (μg/m³) corrected     18 non-null     float64       \n",
            " 11  0.3μm particle count        18 non-null     int64         \n",
            " 12  CO2 (ppm) raw               18 non-null     int64         \n",
            " 13  CO2 (ppm) corrected         18 non-null     int64         \n",
            " 14  Temperature (°C) raw        18 non-null     float64       \n",
            " 15  Temperature (°C) corrected  18 non-null     float64       \n",
            " 16  Heat Index (°C)             18 non-null     float64       \n",
            " 17  Humidity (%) raw            18 non-null     int64         \n",
            " 18  Humidity (%) corrected      18 non-null     int64         \n",
            " 19  TVOC (ppb)                  18 non-null     int64         \n",
            " 20  TVOC index                  18 non-null     int64         \n",
            " 21  NOX index                   18 non-null     int64         \n",
            " 22  PM1 (μg/m³)                 18 non-null     float64       \n",
            " 23  PM10 (μg/m³)                18 non-null     float64       \n",
            " 24  iqa                         18 non-null     float64       \n",
            "dtypes: bool(1), datetime64[ns](1), float64(9), int64(10), object(4)\n",
            "memory usage: 3.5+ KB\n",
            "None\n",
            "   Location ID Location Name  Location Group Location Type  \\\n",
            "0       164928   Breath4life             NaN       Outdoor   \n",
            "1       164928   Breath4life             NaN       Outdoor   \n",
            "2       164928   Breath4life             NaN       Outdoor   \n",
            "3       164928   Breath4life             NaN       Outdoor   \n",
            "4       164928   Breath4life             NaN       Outdoor   \n",
            "\n",
            "                  Sensor ID  Place Open      Local Date/Time UTC Date/Time  \\\n",
            "0  airgradient:d83bda1d43d8        True  2025-08-08 00:00:00    2025-08-08   \n",
            "1  airgradient:d83bda1d43d8        True  2025-08-07 00:00:00    2025-08-07   \n",
            "2  airgradient:d83bda1d43d8        True  2025-08-06 00:00:00    2025-08-06   \n",
            "3  airgradient:d83bda1d43d8        True  2025-08-05 00:00:00    2025-08-05   \n",
            "4  airgradient:d83bda1d43d8        True  2025-08-04 00:00:00    2025-08-04   \n",
            "\n",
            "   # of aggregated records  PM2.5 (μg/m³) raw  ...  \\\n",
            "0                     1173               15.4  ...   \n",
            "1                     1315                7.9  ...   \n",
            "2                     1348                8.3  ...   \n",
            "3                     1312               14.2  ...   \n",
            "4                     1252               13.5  ...   \n",
            "\n",
            "   Temperature (°C) corrected  Heat Index (°C)  Humidity (%) raw  \\\n",
            "0                        30.2             37.6                56   \n",
            "1                        30.6             38.9                57   \n",
            "2                        31.5             40.7                54   \n",
            "3                        30.1             37.5                56   \n",
            "4                        30.6             37.5                53   \n",
            "\n",
            "   Humidity (%) corrected  TVOC (ppb)  TVOC index  NOX index  PM1 (μg/m³)  \\\n",
            "0                      78         123         120          1          5.6   \n",
            "1                      79          89          91          1          2.7   \n",
            "2                      76         121         115          1          3.7   \n",
            "3                      78         133         123          1          6.1   \n",
            "4                      74         143         135          1          6.0   \n",
            "\n",
            "   PM10 (μg/m³)     iqa  \n",
            "0          18.1  133.08  \n",
            "1           9.4  131.17  \n",
            "2           9.3  126.31  \n",
            "3          16.2  130.71  \n",
            "4          15.2  122.53  \n",
            "\n",
            "[5 rows x 25 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer colonnes inutiles\n",
        "cols_a_supprimer = [\n",
        "    'Location ID', 'Location Name', 'Location Group', 'Location Type',\n",
        "    'Sensor ID', 'Place Open', 'UTC Date/Time','# of aggregated records'] + [col for col in merged.columns if \"raw\" in col.lower()]  # remove raw\n",
        "\n",
        "merged.drop(columns=cols_a_supprimer, inplace=True, errors='ignore')"
      ],
      "metadata": {
        "id": "pEgg9ia1xPeh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJri-h_AxPa4",
        "outputId": "d6ee1483-a68a-433b-8121-a6eff4b6a2d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18 entries, 0 to 17\n",
            "Data columns (total 13 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   Local Date/Time             18 non-null     object \n",
            " 1   PM2.5 (μg/m³) corrected     18 non-null     float64\n",
            " 2   0.3μm particle count        18 non-null     int64  \n",
            " 3   CO2 (ppm) corrected         18 non-null     int64  \n",
            " 4   Temperature (°C) corrected  18 non-null     float64\n",
            " 5   Heat Index (°C)             18 non-null     float64\n",
            " 6   Humidity (%) corrected      18 non-null     int64  \n",
            " 7   TVOC (ppb)                  18 non-null     int64  \n",
            " 8   TVOC index                  18 non-null     int64  \n",
            " 9   NOX index                   18 non-null     int64  \n",
            " 10  PM1 (μg/m³)                 18 non-null     float64\n",
            " 11  PM10 (μg/m³)                18 non-null     float64\n",
            " 12  iqa                         18 non-null     float64\n",
            "dtypes: float64(6), int64(6), object(1)\n",
            "memory usage: 2.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Mi8bA-RxPYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictionnnn"
      ],
      "metadata": {
        "id": "_zKc40V80XBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prédiction IQA J+1 avec XGBoost en utilisant TOUTES les features disponibles\n",
        "# Hypothèse pour la prévision J+1→J+5 : les variables exogènes (PM, CO2, etc.)\n",
        "# suivent une persistance (valeur du dernier jour connue). On entraîne sur des lags.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.base import clone\n",
        "import joblib\n",
        "\n",
        "# ========= 1) Préparation =========\n",
        "# df_raw : DataFrame que tu viens d’afficher (13 colonnes)\n",
        "df_raw = merged.copy()  # mets ici ton DataFrame (celui avec 'Local Date/Time' et toutes les mesures)\n",
        "\n",
        "# Passer la date en datetime et trier\n",
        "df_raw['Local Date/Time'] = pd.to_datetime(df_raw['Local Date/Time'])\n",
        "df_raw = df_raw.sort_values('Local Date/Time').reset_index(drop=True)\n",
        "\n",
        "# (Optionnel) forcer un pas journalier et combler s'il manque des jours\n",
        "df_raw = df_raw.set_index('Local Date/Time').asfreq('D')\n",
        "# Interpolation simple pour combler d’éventuels trous\n",
        "for c in df_raw.columns:\n",
        "    df_raw[c] = df_raw[c].interpolate()\n",
        "\n",
        "df_raw = df_raw.reset_index().rename(columns={'Local Date/Time': 'date'})\n",
        "\n",
        "# ========= 2) Lags =========\n",
        "# On crée des lags pour TOUTES les features exogènes + plusieurs lags pour la cible\n",
        "target_col = 'iqa'\n",
        "exog_cols = [c for c in df_raw.columns if c not in ['date', target_col]]\n",
        "\n",
        "# Paramètres de lags\n",
        "n_lags_target = 1   # lags pour iqa\n",
        "n_lags_exog   = 1   # lags pour les exogènes\n",
        "\n",
        "def make_lags(df_in: pd.DataFrame, target: str, exog: list, n_t: int, n_x: int) -> pd.DataFrame:\n",
        "    df_out = df_in.copy()\n",
        "    # lags de la cible\n",
        "    for k in range(1, n_t + 1):\n",
        "        df_out[f'{target}_lag_{k}'] = df_out[target].shift(k)\n",
        "    # lags des exogènes\n",
        "    for col in exog:\n",
        "        for k in range(1, n_x + 1):\n",
        "            df_out[f'{col}_lag_{k}'] = df_out[col].shift(k)\n",
        "    return df_out\n",
        "\n",
        "df_lags = make_lags(df_raw, target_col, exog_cols, n_lags_target, n_lags_exog).dropna().reset_index(drop=True)\n",
        "\n",
        "# ========= 3) Train / Test =========\n",
        "# On prédit iqa (courant) à partir des lags (donc J est prédit par J-1, J-2, ...)\n",
        "features = [c for c in df_lags.columns if c not in ['date', target_col]]\n",
        "X = df_lags[features]\n",
        "y = df_lags[target_col]\n",
        "\n",
        "train_size = int(len(df_lags) * 0.8)\n",
        "X_train, y_train = X.iloc[:train_size], y.iloc[:train_size]\n",
        "X_test,  y_test  = X.iloc[train_size:], y.iloc[train_size:]\n",
        "\n",
        "# ========= 4) CV + Entraînement =========\n",
        "n_splits = max(2, min(5, len(X_train) - 1))\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "base_xgb = XGBRegressor(\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.04,\n",
        "    max_depth=3,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    eval_metric=\"rmse\",  # compatible v2+ (défini dans le constructeur)\n",
        ")\n",
        "\n",
        "# CV manuelle (pas d'early_stopping pour compat toutes versions)\n",
        "cv_rmses = []\n",
        "for tr_idx, val_idx in tscv.split(X_train):\n",
        "    est = clone(base_xgb)\n",
        "    est.fit(X_train.iloc[tr_idx], y_train.iloc[tr_idx], verbose=False)\n",
        "    y_val_hat = est.predict(X_train.iloc[val_idx])\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_train.iloc[val_idx], y_val_hat)))\n",
        "    cv_rmses.append(rmse)\n",
        "\n",
        "cv_rmses = np.array(cv_rmses)\n",
        "print(\"📊 CV RMSE per fold :\", np.round(cv_rmses, 4))\n",
        "print(\"📊 CV RMSE mean     :\", np.round(cv_rmses.mean(), 4))\n",
        "\n",
        "# Entraînement final sur tout le train\n",
        "final_xgb = clone(base_xgb)\n",
        "final_xgb.fit(X_train, y_train, verbose=False)\n",
        "\n",
        "# ========= 5) Évaluation =========\n",
        "y_pred = final_xgb.predict(X_test)\n",
        "rmse_test = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "mae_test  = float(mean_absolute_error(y_test, y_pred))\n",
        "r2_test   = float(r2_score(y_test, y_pred))\n",
        "print(\"🔎 Évaluation sur test :\")\n",
        "print(f\"   RMSE = {rmse_test:.4f}\")\n",
        "print(f\"   MAE  = {mae_test:.4f}\")\n",
        "print(f\"   R2   = {r2_test:.4f}\")\n",
        "\n",
        "# ========= 6) Sauvegarde =========\n",
        "joblib.dump(final_xgb, \"xgb_iqa_all_features.pkl\")\n",
        "print(\"✅ Modèle sauvegardé : xgb_iqa_all_features.pkl\")\n",
        "\n",
        "# ========= 7) Prévision J+1 =========\n",
        "# On part de la dernière ligne connue et de ses lags déjà construits\n",
        "last_row = X.iloc[[-1]]  # DataFrame 1xN\n",
        "pred_j1 = float(final_xgb.predict(last_row)[0])\n",
        "print(f\"📅 Prédiction J+1 : {pred_j1:.2f}\")\n",
        "\n",
        "# ========= 8) Prévisions J+1 → J+5 (auto-régression) =========\n",
        "# Règle de persistance pour les exogènes : leurs lags avancent en gardant la dernière valeur connue.\n",
        "# Les lags de iqa se mettent à jour avec les prédictions successives.\n",
        "\n",
        "n_days = 5\n",
        "multi_preds = []\n",
        "\n",
        "# On travaille sur une copie des features de la dernière ligne\n",
        "step_feats = last_row.copy()\n",
        "\n",
        "# Colonnes de lags cible et exogènes\n",
        "iqa_lag_cols = [f'{target_col}_lag_{k}' for k in range(1, n_lags_target + 1)]\n",
        "exog_lag_cols = []\n",
        "for col in exog_cols:\n",
        "    for k in range(1, n_lags_exog + 1):\n",
        "        exog_lag_cols.append(f'{col}_lag_{k}')\n",
        "\n",
        "for _ in range(n_days):\n",
        "    # prédire\n",
        "    y_hat = float(final_xgb.predict(step_feats)[0])\n",
        "    multi_preds.append(y_hat)\n",
        "\n",
        "    # 1) MAJ lags iqa : shift à droite et mettre lag_1 = y_hat\n",
        "    iqa_vals = step_feats[iqa_lag_cols].to_numpy().ravel()\n",
        "    iqa_vals = np.roll(iqa_vals, 1)\n",
        "    iqa_vals[0] = y_hat\n",
        "    step_feats[iqa_lag_cols] = iqa_vals\n",
        "\n",
        "    # 2) MAJ lags exogènes : persistance (on répète la dernière valeur)\n",
        "    if len(exog_lag_cols) > 0:\n",
        "        exog_vals = step_feats[exog_lag_cols].to_numpy().ravel()\n",
        "        exog_vals = np.roll(exog_vals, 1)\n",
        "        # pour persistance, on remet exog_lag_1 = exog_lag_1 actuel (avant shift) → on récupère l'ancienne valeur\n",
        "        # Ici, plus simple : on garde la valeur après shift (équivaut à persistance de la dernière connue)\n",
        "        step_feats[exog_lag_cols] = exog_vals\n",
        "\n",
        "print(\"📅 Prédictions J+1 à J+5 :\", [round(p, 2) for p in multi_preds])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKhbtao0wz-i",
        "outputId": "ff7466b8-eea3-46fc-f529-3dc7cfc85756"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 CV RMSE per fold : [9.8528 3.171  4.2602 3.4989 1.3899]\n",
            "📊 CV RMSE mean     : 4.4346\n",
            "🔎 Évaluation sur test :\n",
            "   RMSE = 1.4155\n",
            "   MAE  = 0.9035\n",
            "   R2   = 0.8629\n",
            "✅ Modèle sauvegardé : xgb_iqa_all_features.pkl\n",
            "📅 Prédiction J+1 : 130.04\n",
            "📅 Prédictions J+1 à J+5 : [130.04, 129.66, 130.02, 129.55, 128.94]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYbVC6GGzTHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YQMW0xO9zTDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j7XBEoFQzTBV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}