import requests
import os
import pandas as pd
from datetime import datetime
from urllib.parse import urlencode


#=============================================================================================================


#   üß± Architecture du module api_handler.py

#  ‚úÖ 2. Fonction : fetch_latest_data(location_id)
#  ‚Äì R√©cup√®re la derni√®re donn√©e disponible pour une location , il s'agit de la donnee de la journee (Une aggregation de toutes les donnees collectees la journee , donc en fait c'est juste une seule ligne ).
#  ‚Äì Utilise :
#  GET /public/api/v1/locations/{location_id}/measures/current
#  ‚Üí Elle sera utilis√©e chaque jour a 23h59 pour recuperer les donnees de la journee .

#  ‚úÖ 3. Fonction : append_new_data(location_id)
#  ‚Äì Compare la derni√®re ligne enregistr√©e localement avec les nouvelles donn√©es de l‚ÄôAPI.
#  ‚Äì Si nouvelle, ajoute au fichier CSV de la location dans /data.

#  ‚úÖ 4. Fonction : save_data_to_csv(data, location_id)
#  ‚Äì Enregistre ou met √† jour un fichier CSV contenant toutes les donn√©es de cette location.

#=============================================================================================================


BASE_URL = "https://api.airgradient.com/public/api/v1"
DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data/now')

#=============================================================================================================


def ping_server():
    """Teste la connexion √† l‚ÄôAPI AirGradient."""
    url = f"{BASE_URL}/ping"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return True
        else:
            return False
    except requests.exceptions.RequestException as e:
        print(f"Erreur lors de la connexion √† l'API : {e}")
        return False

#=============================================================================================================

def save_data_to_csv(df: pd.DataFrame, location_id: str):
    """
    Sauvegarde un DataFrame dans un fichier CSV nomm√© selon la location.
    √âcrase le fichier s‚Äôil existe d√©j√†.
    """
    
    file_path = os.path.join(DATA_DIR, f"{location_id}.csv")
    df.to_csv(file_path, index=False)

#=============================================================================================================

def fetch_latest_data(location_id: str, token: str) -> pd.DataFrame:
    """
    R√©cup√®re la donn√©e journali√®re agr√©g√©e (bucketSize=1day) pour aujourd'hui √† partir de l'endpoint /measures/past.
    """

    endpoint = f"/locations/{location_id}/measures/past"

    today = datetime.utcnow().strftime("%Y-%m-%d")
    from_param = f"{today}T00:00:00Z"
    to_param = f"{today}T23:59:59Z"

    params = {
        "token": token,
        "from": from_param,
        "to": to_param
    }

    full_url = f"{BASE_URL}{endpoint}?{urlencode(params)}"

    try:
        response = requests.get(full_url)
        response.raise_for_status()
        data = response.json()

        return pd.DataFrame(data) if isinstance(data, list) else pd.DataFrame()
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration de la donn√©e journali√®re pour {location_id} : {e}")
        return pd.DataFrame()


#=============================================================================================================

def append_new_data(location_id: str, new_data: dict):
    """
    Ajoute les nouvelles donn√©es (dict) au fichier CSV existant pour la location,
    uniquement si elles ne sont pas d√©j√† pr√©sentes (bas√© sur le timestamp).
    """
    
    file_path = os.path.join(DATA_DIR, f"{location_id}.csv")

    # Charger l'ancien CSV s'il existe
    if os.path.exists(file_path):
        df_old = pd.read_csv(file_path)
    else:
        df_old = pd.DataFrame()

    # Convertir la nouvelle donn√©e en DataFrame
    df_new = pd.DataFrame([new_data])

    # V√©rifier si le timestamp est d√©j√† pr√©sent
    if not df_old.empty and 'timestamp' in df_old.columns and new_data.get("timestamp") in df_old["timestamp"].values:
        print("‚ÑπÔ∏è Donn√©e d√©j√† pr√©sente, pas d'ajout.")
        return

    # Ajouter et sauvegarder
    df_combined = pd.concat([df_old, df_new], ignore_index=True)
    df_combined.to_csv(file_path, index=False)


#=============================================================================================================

def generate_daily_summary(location_id, file_path):
    try:
        # V√©rifie si le fichier existe et n'est pas vide
        if os.path.getsize(file_path) == 0:
            print(f"[‚ö†Ô∏è] Fichier vide ignor√© : {file_path}")
            return

        df = pd.read_csv(file_path)

        if df.empty:
            print(f"[‚ö†Ô∏è] Aucune donn√©e dans le fichier : {file_path}")
            return

        # Filtrer les donn√©es par locationId
        df = df[df['locationId'] == location_id]

        if df.empty:
            print(f"[‚ö†Ô∏è] Aucune donn√©e pour locationId={location_id} dans {file_path}")
            return

        # Colonnes fixes √† conserver
        categorical_cols = df.select_dtypes(include='object').columns
        numerical_cols = df.select_dtypes(include='number').columns.difference(categorical_cols)

        # Moyenne des colonnes num√©riques
        mean_values = df[numerical_cols].mean()
        constant_values = df[categorical_cols].iloc[0]

        # Fusion et export
        result = pd.concat([constant_values, mean_values]).to_frame().T
        output_filename = f"{location_id}.csv"
        result.to_csv(output_filename, index=False)

        print(f"[‚úÖ] Moyenne journali√®re export√©e dans : {output_filename}")

    except pd.errors.EmptyDataError:
        print(f"[‚ö†Ô∏è] Fichier vide (aucune colonne) : {file_path}")
    except FileNotFoundError:
        print(f"[‚ùå] Fichier introuvable : {file_path}")
    except Exception as e:
        print(f"[‚ùå] Erreur inattendue pour {file_path} : {e}")
